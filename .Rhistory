library(knitr)
library(rmdformats)
library(DT)
# KNN classifier algorithm (multivariate, multiclass) -- probability version
#   x0 = length d vector, the new x point at which to predict the y value.
#   x = d-by-n matrix of training x's, where x[,i] is the i'th training point x_i.
#   y = (y[1],...,y[n]) = vector of training y's, where y[i] is in {1,...,C}.
#   K = number of neighbors to use.
#   C = number of classes.
#   p_hat = (p_hat[1],...,p_hat[C]) where p_hat[j] = estimated probability of y0=j given x0.
KNN_multi = function(x0, x, y, K, C) {
# 1. Compute euclidean distance between x0 and each x[,i]
distances <- sqrt(colSums((x0 - x)^2))
# 2. order of the training points by distance from x0 (nearest to farthest)
o <- order(distances)
# 3. p_hat[j] = proportion of y values of the K nearest training points that are equal to j.
p_hat <- sapply(1:C, function(j){ sum(y[o[1:K]]==j)/K })
# 4. return estimated probabilities
return(p_hat)
}
# KNN classifier algorithm (multivariate, multiclass) -- prediction version
KNN_multi_predict = function(x0, x, y, K, C) {
# 1. compute the estimated probabilities
p_hat <- KNN_multi(x0, x, y, K, C)
# 2. find the class with the highest estimated probability
y0 <- which.max(p_hat)
# 3. return the predicted class
return(y0)
}
# Simulate training and test data
# reset the random number generator
set.seed(1)
# dimension of each training point x_i- set this to (a) 2 and (b) 20
d = 2
# number of training samples to simulate
n = 100
# number of test samples to simulate
n_test = 10000
# true relationship between x's and y's
f = function(x0) { (x0[1]>0)+(x0[2]>0)+1 }
# simulate matrix of training x's
x = matrix(rnorm(n*d),d,n)
# simulate training y's with no noise
y = apply(x,2,f)
# simulate test x's
x_test = matrix(rnorm(n_test*d),d,n_test)
# simulate test y's
y_test = apply(x_test,2,f)
# plot dimensions 1 and 2
plot(x[1,],x[2,],col=y+1,pch=19)
# Apply your KNN classifier with K=5
# number of classes
C = 3
# number of neighbors to use
K = 5
# predictions on the training set
y_hat = apply(x, 2, function(x0) { KNN_multi_predict(x0, x, y, K, C) })
# predictions on the test set
y_test_hat = apply(x_test, 2, function(x0) { KNN_multi_predict(x0, x, y, K, C) })
# compute the training error rate
train_error = mean(y_hat != y)
# compute the test error rate
test_error = mean(y_test_hat != y_test)
print(data.table('error type'=c('train','test'),'value'=c(train_error,test_error)))
library(knitr)
library(rmdformats)
library(DT)
# Simulate training and test data
# reset the random number generator
set.seed(1)
# dimension of each training point x_i- set this to (a) 2 and (b) 20
d = 2
# number of training samples to simulate
n = 100
# number of test samples to simulate
n_test = 10000
# true relationship between x's and y's
f = function(x0) { (x0[1]>0)+(x0[2]>0)+1 }
# simulate matrix of training x's
x = matrix(rnorm(n*d),d,n)
# simulate training y's with no noise
y = apply(x,2,f)
# simulate test x's
x_test = matrix(rnorm(n_test*d),d,n_test)
# simulate test y's
y_test = apply(x_test,2,f)
# plot dimensions 1 and 2
plot(x[1,],x[2,],col=y+1,pch=19)
# Apply your KNN classifier with K=5
# number of classes
C = 3
# number of neighbors to use
K = 5
# predictions on the training set
y_hat = apply(x, 2, function(x0) { KNN_multi_predict(x0, x, y, K, C) })
# predictions on the test set
y_test_hat = apply(x_test, 2, function(x0) { KNN_multi_predict(x0, x, y, K, C) })
# compute the training error rate
train_error = mean(y_hat != y)
# compute the test error rate
test_error = mean(y_test_hat != y_test)
print(data.table('error type'=c('train','test'),'value'=c(train_error,test_error)))
library(knitr)
library(rmdformats)
library(DT)
library(data.table)
# Simulate training and test data
# reset the random number generator
set.seed(1)
# dimension of each training point x_i- set this to (a) 2 and (b) 20
d = 2
# number of training samples to simulate
n = 100
# number of test samples to simulate
n_test = 10000
# true relationship between x's and y's
f = function(x0) { (x0[1]>0)+(x0[2]>0)+1 }
# simulate matrix of training x's
x = matrix(rnorm(n*d),d,n)
# simulate training y's with no noise
y = apply(x,2,f)
# simulate test x's
x_test = matrix(rnorm(n_test*d),d,n_test)
# simulate test y's
y_test = apply(x_test,2,f)
# plot dimensions 1 and 2
plot(x[1,],x[2,],col=y+1,pch=19)
# Apply your KNN classifier with K=5
# number of classes
C = 3
# number of neighbors to use
K = 5
# predictions on the training set
y_hat = apply(x, 2, function(x0) { KNN_multi_predict(x0, x, y, K, C) })
# predictions on the test set
y_test_hat = apply(x_test, 2, function(x0) { KNN_multi_predict(x0, x, y, K, C) })
# compute the training error rate
train_error = mean(y_hat != y)
# compute the test error rate
test_error = mean(y_test_hat != y_test)
print(data.table('error type'=c('train','test'),'value'=c(train_error,test_error)))
print(data.table('error type'=c('train','test'),'value'=c(train_error,test_error)))
# Simulate training and test data
# reset the random number generator
set.seed(1)
# dimension of each training point x_i- set this to (a) 2 and (b) 20
d = 20
# number of training samples to simulate
n = 100
# number of test samples to simulate
n_test = 10000
# true relationship between x's and y's
f = function(x0) { (x0[1]>0)+(x0[2]>0)+1 }
# simulate matrix of training x's
x = matrix(rnorm(n*d),d,n)
# simulate training y's with no noise
y = apply(x,2,f)
# simulate test x's
x_test = matrix(rnorm(n_test*d),d,n_test)
# simulate test y's
y_test = apply(x_test,2,f)
# plot dimensions 1 and 2
plot(x[1,],x[2,],col=y+1,pch=19)
# Apply your KNN classifier with K=5
# number of classes
C = 3
# number of neighbors to use
K = 5
# predictions on the training set
y_hat = apply(x, 2, function(x0) { KNN_multi_predict(x0, x, y, K, C) })
# predictions on the test set
y_test_hat = apply(x_test, 2, function(x0) { KNN_multi_predict(x0, x, y, K, C) })
# compute the training error rate
train_error = mean(y_hat != y)
# compute the test error rate
test_error = mean(y_test_hat != y_test)
print(data.table('error type'=c('train','test'),'value'=c(train_error,test_error)))
# Simulate training and test data
# reset the random number generator
set.seed(1)
# dimension of each training point x_i- set this to (a) 2 and (b) 20
d = 20
# number of training samples to simulate
n = 100
# number of test samples to simulate
n_test = 10000
# true relationship between x's and y's
f = function(x0) { (x0[1]>0)+(x0[2]>0)+1 }
# simulate matrix of training x's
x = matrix(rnorm(n*d),d,n)
# simulate training y's with no noise
y = apply(x,2,f)
# simulate test x's
x_test = matrix(rnorm(n_test*d),d,n_test)
# simulate test y's
y_test = apply(x_test,2,f)
# plot dimensions 1 and 2
plot(x[1,],x[2,],col=y+1,pch=19)
# Apply your KNN classifier with K=5
# number of classes
C = 3
# number of neighbors to use
K = 20
# predictions on the training set
y_hat = apply(x, 2, function(x0) { KNN_multi_predict(x0, x, y, K, C) })
# predictions on the test set
y_test_hat = apply(x_test, 2, function(x0) { KNN_multi_predict(x0, x, y, K, C) })
# compute the training error rate
train_error = mean(y_hat != y)
# compute the test error rate
test_error = mean(y_test_hat != y_test)
print(data.table('error type'=c('train','test'),'value'=c(train_error,test_error)))
# Compute test error when we always naively predict y0=2, regardless of x0.
naive_test_error = mean(2 != y_test)
# Compute test error when we make the optimal prediction f(x0).
optimal_test_error = mean(apply(x_test,2,f) != y_test)
# Simulate training and test data
# reset the random number generator
set.seed(1)
# dimension of each training point x_i- set this to (a) 2 and (b) 20
d = 2
# number of training samples to simulate
n = 100
# number of test samples to simulate
n_test = 10000
# true relationship between x's and y's
f = function(x0) { (x0[1]>0)+(x0[2]>0)+1 }
# simulate matrix of training x's
x = matrix(rnorm(n*d),d,n)
# simulate training y's with no noise
y = apply(x,2,f)
# simulate test x's
x_test = matrix(rnorm(n_test*d),d,n_test)
# simulate test y's
y_test = apply(x_test,2,f)
# plot dimensions 1 and 2
plot(x[1,],x[2,],col=y+1,pch=19)
# Apply your KNN classifier with K=5
# number of classes
C = 3
# number of neighbors to use
K = 5
# predictions on the training set
y_hat = apply(x, 2, function(x0) { KNN_multi_predict(x0, x, y, K, C) })
# predictions on the test set
y_test_hat = apply(x_test, 2, function(x0) { KNN_multi_predict(x0, x, y, K, C) })
# compute the training error rate
train_error = mean(y_hat != y)
# compute the test error rate
test_error = mean(y_test_hat != y_test)
print(data.table('error type'=c('train','test'),'value'=c(train_error,test_error)))
# Compute test error when we always naively predict y0=2, regardless of x0.
naive_test_error = mean(2 != y_test)
# Compute test error when we make the optimal prediction f(x0).
optimal_test_error = mean(apply(x_test,2,f) != y_test)
# (Change d above and rerun the code for each case.)
plot(x_test[1,1:100],x_test[2,1:100],col=y_test_hat[1:100]+1,pch=19)  # show dims 1 and 2 only
# Load the gene expression data
Dx = read.table(file="lab-1-gene-expression.txt", header=TRUE, sep='\t', row.names=1)
# Load the gene expression data
Dx = read.table(file="lab-1-gene-expression.txt", header=TRUE, sep='\t', row.names=1)
x = sapply(Dx, as.numeric)
# Load the cancer subtype labels
Dy = read.table(file="lab-1-leukemia-type.txt", header=TRUE, sep='\t', row.names=1)
y = as.numeric(factor(Dy[,1]))
# Split into training and test subsets
n = length(y)  # number of samples
set.seed(1)  # reset the random number generator
train = sample(1:n, round(n/2))  # random subset to use as training set
test = setdiff(1:n, train)  # subset to use as test set
# Run your KNN classifier with (a) K=5 and (b) K=30 to make predictions on the test set x[,test]
# based on the training data in x[,train] and y[train].  Compute the error rate on the test set by
# comparing your predictions with y[test]. Report your results below, using the same train/test split
# for both K=5 and K=30.
C = 3  # number of classes
K = 5  # number of neighbors to use
y_test_hat = apply(x[,test], 2, function(x0) { KNN_multi_predict(x0, x[,train], y[,train], K, C) })  # predictions on the test set
View(x)
y_test_hat = apply(x[,test], 2, function(x0) { KNN_multi_predict(x0, x[,train], y[train], K, C) })  # predictions on the test set
test_error = mean(y_test_hat != y[test])  # compute the error rate on the test set
# Load the gene expression data
Dx = read.table(file="lab-1-gene-expression.txt", header=TRUE, sep='\t', row.names=1)
x = sapply(Dx, as.numeric)
# Load the cancer subtype labels
Dy = read.table(file="lab-1-leukemia-type.txt", header=TRUE, sep='\t', row.names=1)
y = as.numeric(factor(Dy[,1]))
# Split into training and test subsets
n = length(y)  # number of samples
set.seed(1)  # reset the random number generator
train = sample(1:n, round(n/2))  # random subset to use as training set
test = setdiff(1:n, train)  # subset to use as test set
# Run your KNN classifier with (a) K=5 and (b) K=30 to make predictions on the test set x[,test]
# based on the training data in x[,train] and y[train].  Compute the error rate on the test set by
# comparing your predictions with y[test]. Report your results below, using the same train/test split
# for both K=5 and K=30.
C = 3  # number of classes
K = 30  # number of neighbors to use
y_test_hat = apply(x[,test], 2, function(x0) { KNN_multi_predict(x0, x[,train], y[train], K, C) })  # predictions on the test set
test_error = mean(y_test_hat != y[test])  # compute the error rate on the test set
# Report your results:
# (a) K=5
#    test error rate = 0.05
#
# (b) K=30
#    test error rate = YOUR_ANSWER_HERE
# Load the gene expression data
Dx = read.table(file="lab-1-gene-expression.txt", header=TRUE, sep='\t', row.names=1)
x = sapply(Dx, as.numeric)
# Load the cancer subtype labels
Dy = read.table(file="lab-1-leukemia-type.txt", header=TRUE, sep='\t', row.names=1)
y = as.numeric(factor(Dy[,1]))
# Split into training and test subsets
n = length(y)  # number of samples
set.seed(1)  # reset the random number generator
train = sample(1:n, round(n/2))  # random subset to use as training set
test = setdiff(1:n, train)  # subset to use as test set
# Run your KNN classifier with (a) K=5 and (b) K=30 to make predictions on the test set x[,test]
# based on the training data in x[,train] and y[train].  Compute the error rate on the test set by
# comparing your predictions with y[test]. Report your results below, using the same train/test split
# for both K=5 and K=30.
C = 3  # number of classes
K = 5  # number of neighbors to use
y_test_hat = apply(x[,test], 2, function(x0) { KNN_multi_predict(x0, x[,train], y[train], K, C) })  # predictions on the test set
test_error = mean(y_test_hat != y[test])  # compute the error rate on the test set
# Report your results:
# (a) K=5
#    test error rate = 0.05
#
# (b) K=30
#    test error rate = 0.694
y
